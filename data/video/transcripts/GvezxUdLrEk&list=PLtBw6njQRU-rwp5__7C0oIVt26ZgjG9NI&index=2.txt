all right well it's fantastic to have everyone here my name is Ava I'm also an instructor for the course and this is now lecture two of 6s one91 on deep sequence modeling and as Alexander kind of alluded to at the end of his lecture deep sequence modeling is a very very powerful concept that especially as of late has gone a lot of interest and a lot of excitement around it due to the Advent of large language models and actually excitingly in the course this year we're going to have two two and a half uh lectures guest lectures on large language models on Thursday and Friday and Our intention with this lecture is to really give you the foundations of what sequence modeling is all about so that you could be ready when we go into these lectures on these Cutting Edge New Frontier topics to really grasp them from the fundamentals and Foundations up okay so with that in mind in the first lecture Alexander introduced the very Basics the essentials of neural networks how we train them using back propagation what it means to define a feedforward model and so in this next talk we're going to turn our attention to now applying these neural networks to problems that involve sequential processing or sequential modeling of data and what we'll do is we'll try to walk through this problem formulation and the models step by step side by side building up our intuition about how we can build these networks up starting right where we left off in the end of last lecture so to first set the stage what I always like to do is to motivate this notion of sequence modeling and sequential data with a very very intuitive example super simple one let's say we have this image of a ball in 2D space and our goal is to predict where the ball is going to travel to next now if I didn't give you any prior information about the ball's history its motion in this 2D space any guess you place on the next position of the ball will be a random guess now instead if in addition to the current location of the ball I gave you some information about its history say its prior locations now your problem becomes much easier and effectively the task is reduced to given this past history about the ball predict where it's going to go to next and I think we can all agree that in this example right the ball is appearing that it's moving from left to right so this is this notion of what we mean when we think think about sequence modeling and Beyond this simple example as we'll see throughout the course sequential data and sequence modeling is really really all around us for example the audio from my voice talking can be split up into a sequence of sound waves effectively chunked and processed in a sequence we can do this similarly with words and texts and natural language where we can chunk or split up text into a sequence of individual character characters individual words and process that in that manner Beyond this there's many many more cases in which sequential processing becomes Apparent from medical signals like ECGs to stock prices in financial markets to biological sequences like nucleic acids or protein sequences to weather to motion video and more and so really this P this Paradigm of sequence modeling unlocks a great potential of applications and real world use cases for deep learning in the wild so what are some concrete problem formulations or tasks we can think about when we start to think about sequence modeling in the first lecture we learned about kind of the very basic prediction problem like a classification problem of will I pass this class yes or no binary now instead of being able to go from a single input to a single output like a class label with sequence modeling we want to process a sequence of inputs such as the text the words in an individual sentence and produce a output at the end which may be similarly a classification label like does a sentence have a positive or negative sentiment or feeling associated with it it could also be a sequential output like taking an image and now producing or generating text that captions that image or similarly we can think about many to many generative or predictive tasks where maybe we have sequences in one language in English and we want to translate that into sequences of another language so these These are kind of the the fundamental problem definitions or problem setups that we can start to think about when we have the capability to process sequential data so now with that in hand the question becomes okay how do we actually build up a neural network model to be able to process and handle this unique type of sequential data and today we'll spend the about half the time in the lecture talking about where kind of the history of the field got started with a new type of neural network architecture that kicked off our our real abilities to model sequences and then in the second half we'll talk about a powerful mechanism that is being used today in the most state-of-the-art um sequence models that we see all around us in both cases my goal is really to convey the fundamentals of how we actually do this and so we're going to build up from the perceptron that Alexander introduced and go step by step to develop an understanding of these models okay so let's do exactly that let's go back to the perceptron which we studied in lecture one right we saw a diagram like this where we have a set of inputs X1 through XM and these are numbers right they're represented numerically and what we do in a perceptron is we take these inputs we multiply it by a corresponding weight value add them all together to get this state Z and take this Z and pass it through a function that's nonlinear also called an activation function and that leads us to our predicted output YH while we can have multiple inputs coming in here you can think of these inputs as a single slice or as a single time step in a sequence we we also saw that we are able to stack these individual perceptrons together to comprise a layer of a neural network and here we can also extend to go from a set of uh inputs to a set of multiple outputs in purple but still right in this example we're still operating on a static single step right there's no notion of sequence or no notion of time these inputs are all these individual inputs at just one slice of a Time step we can simplify our diagram further and all I'm doing here in the middle is that I've collapsed that visual of the Hidden layer down to just this abstracted green box still we have the same operation of mapping an input to an output some vectors of length M two output vectors of length n what can we do now to try to apply a network like this to time series data or sequential data well maybe let's start off with something perhaps a bit naive as a first pass let's take this same model all I've done here is I've just rotate it so it's instead of left to right it's down to up input to output and what if we repeat this multiple times we have our input Vector at some time step T here t0 we feed it into our neural network and we generate an output prediction and maybe we do this over multiple time steps because in sequential data we don't have just a single time step we have multiple individual time steps from t0 to T1 T2 and so on in the sequence we could do this in isolation right we take the inputs at these individual time steps pass them in through the model and generate an output and here again it's a simple function where we're applying this function defined by our neural network to the input at a time step T to generate a prediction y hat of T but what's the problem here right we know that our output Vector y hat at a particular time step is just going to be a function of the input at that time step and if this is inherently sequential data it's probably in sequence for a reason because there's some sort of inherent dependence on time steps within that sequence relative to each other and in this way we're completely ignoring the inputs at the previous time steps to make a prediction at a later time step right these are all treated in isolation to predict a Time Step at say time an output at say time two we could probably benefit aot lot from taking the information from time Step Zero time step one how do we circumvent this how could we possibly relate these time steps to each other well fundamentally what we're trying to do is to relate the Network's internal computations at these individual time steps to Prior history of the computations from those prior previously observed time steps and we want to pass this information on forward through time to maintain that information as we move through the sequence so maybe we can try that out what if we were able to define a way to link the neuron's internal state from a previous time step to the computation at a later time step maybe we can make this concrete we can define a variable we'll call it an internal State h of T that's maintained by the neurons in this network and we pass this stay on time step to time step across time and our goal is to maybe embibe this this internal state with some notion of memory right some record of the prior computations that the network has made and what this means a little more concretely is now our prediction why hat of T at a particular time step should depend not only on the input at that time step but also this state that's been carried on from the prior time step this past history and the current input and so what's really powerful here is now we're starting to Define more formally a relation between these output predictions that depends not only on the input observed but also the this past memory and as you can kind of see and hopefully start to get intuition for that past memory is dependent on the prior inputs right so we have this idea of some sort of recurrence something that's repeated and maintained over time as the network makes these computations and because this output that we're producing is a function of not only the current input but also this past State we can describe this type of network via what we call a recurrence relation and we can visualize this in a couple of different ways here on the right I'm showing these time steps kind of unrolled in slices over time where we've made these links kind of visualized by this variable H but we can also represent this on the left via this cycle this Loop which tells us this concept of the state maintaining this recurrent log effectively of the Network's internal computations that gets updated with each time step and so this is this notion of a Time series model that has a sense of recurrence and really this is a very very foundational Concept in all of sequence modeling this notion of what we call recurrent neural networks and hopefully this example and this buildup from where we started with the perceptron gives you this intuition about how we can start to model sequential data using these time series models called recurrent neural networks so let's keep going right let's continue to build from this foundation and now start to get a little bit more formal about the operations that go into the internals of these recurrent neural networks or rnns as we started to see right the key idea is that we have this internal State h h of T and that state is going to be updated at each time step as we process the sequence we can do this by defining what we call this recurrence relation right it's a way to process a sequence time step by time step and what we want is our internal CLE State H to to be a function of not only the input but also the prior state so that there's some notion of memory that's carried through to formalize this the cell State h of T is defined by some function parameterized by a set of Weights W that depends not only on the input at that time step but also the prior state from the previous time step and we can recur currently apply the same function time step by time step updating the weights so that we get this uh recurrent update to our cell State as the sequence is processed an additional way to build up this intuition about rnns is to think about this kind of in pseudo code right so here we're going to walk through a python pseudo code example that helps build up this intuition so let's say we want to convey and and build up the uh RNN from scratch we're going to Define some RNN my RNN and we're going to initialize it with a hidden State set to zero and now our input is this little fragment of a sentence I love recurrent neural and our task is to use the individual words in this sentence to predict the next word that comes next in that sequence what we do is we Loop through for each word in this sentence we use our RNN to take that word take the last hidden State predict some output and update the hidden State and do this iteratively right through this iterative procedure such that at the end we can generate a prediction for the next word that comes after we've processed these inputs this is the fundamental of this notion of aate update and output that is the core of the RNN so again walking through this from the from the bottom up given our input Vector we Define a function to update the hidden State and now we've just introduced a little bit more formality that makes these weight definitions and functions uh explicit but just again this is the update to the hidden state is a function of both the input at that time step and the prior hidden state from the previous time step that updates our hidden State and now we can actually generate a prediction an output vector by taking that hidden State and applying awaited transformation to it right so this is the core of both how the RNN can update its internal memory its hidden State and also produce a prediction and output at each individual time step so by seeing this and visualizing this rnm as sort of this recurrent cyclic visual here I mentioned that we can also represent it by kind of taking these individual time slices and separating them out on a Time axis unrolling them across time and if we do that I think the intuition becomes perhaps even more clear where again starting from the the first time step we can unroll step by step T1 T2 all the way to T where you can see this internal state is being passed on time step to time step we're generating the predictions we can also bring back that formal math to this diagram where now you can see we have one weight Matrix that transforms the input to the computation of the Hidden State we have a weight Matrix that defines how the hidden state gets get updated and finally one that transforms the hidden state to the output prediction importantly right these are the same weight matrices at each individual time step right there's this this and this and they're reused at each time step in the sequence so now this tells us okay how do we actually make these updates how do we predict the outputs but what we need to know to actually train this network and learn it is how do we Define a loss right as Alexander introduced in order to learn a neural network or train it you need to Define some sort of function that tells you how close the predictions are actually getting to the desired behavior that you seek out and with rnns that concept that exact same concept still applies the only difference is that we can compute a loss at each slice at each of these individual time steps in the sequence and get a total loss by summing over all the time steps in our sequence so in total right this defines what we call our forward path how we make predictions time step by time step and how we use those predictions to compute a loss we can walk through an example of how you can actually Implement an RN yourself bottom up from scratch in a library like tensorflow where we try to Define an RNN as a layer per uh initializing the weight matrices as attributes of our RNN class and we Define importantly in this function call how we do that exact forward operation that forward pass that I introduced in the prior slide and as you can see right it consists of these two key lines the first being the update of the Hidden State according to the input and the prior hidden State and the second being the prediction of the output which is a transformation of the Hidden State at that time step and we return both of these the prediction and that hidden State value and as you'll see right this is a really good way to move now thinking in code from that pseudo code into inition we showed earlier to now kind of how you can define an RNN class yourself from scratch and moving forward one more step you can build from this intuition to now learn and understand how to operationalize an RNN through uh layers and modules that are already implemented in these common machine learning Frameworks like tensor flow and P torch and in the first lab of this course using uh rnns for sequence modeling you'll get hands-on experience in working um with these these functions and classes in either of these libraries okay so hopefully now right that gives you this intuition of how we've built up from this first example of static input static output to to these diverse types of tasks and new sorts of problems we can tackle when we start to be able to process and handle sequential data whether that's taking a sequence producing a class label or actually being able to do something like next word prediction or next character prediction to actually generate and produce out a sequence as output and in this latter example forecasting a bit not only is this what get hands-on experience with in our software lab but really this notion of many to many sequence modeling is the backbone of how language models actually work and you've just you know kind of understood some of that intuition um starting today and and now Building forward all right so let's now think about how do we operationalize this in the real world yeah go ahead uh every node in the hidden first hidden connected by tode in the second is that how the AR looks yeah so the question is what defines the connection between the hidden layers the important thing is that one of these individual green blocks that can have many layers itself right it doesn't necessarily have to be one it could be many the important thing is that that's you can think of that as a unit and that unit contains some number of layers within that unit but that operation is basically applied for all the individual time steps in that um sequence using the same set of layers and weights in that unit you compute the loss at each of those steps and then as we'll see when you actually update the weights that is done by taking the loss from all the individual time steps and then updating after you've processed all the time steps individually just just in the same in the same way as with feed forward networks where you can have weights that connect layer to layer in RNN that has multiple layers within one unit there is weights there are weights that define those connections any additional questions before we transition otherwise okay okay so now now let's think about how we actually operationalize and bring this this notion of sequence modeling to a real world example sequences are rich and sequences are interesting for a few reasons one is because they're tremendously variable right it's not like an image where you try to have you know a height and a width of the image and for all your images in your data set those height and width are fixed in something like language sequences can be short they can be long they can be in between so we need to be able to handle this variability the other richness and Nuance is that inherently with things that depend on time you could have instances where there's a really short range interaction or dependency or you can have instances in a sequence where something at the very beginning dictates the meaning or uh the value at something at the very end so this notion of spread or long-term dependency and kind of fundamentally right there's this notion of order and our model needs to be able to do a good job of representing and reasoning about this order um through through the approach that we take so we'll use these kind of criteria to to motivate how rnn's can do well at doing this but also what are some shortcomings of rnns when it comes to these kind of real world operational criteria that our sequence model needs to meet so to do that we're going to walk through a very very concrete example which is really maybe now has become the quintessential sequence modeling problem out there and that's this idea of predict the next word given a sequence of words predict the word that comes next and this to underscore this this is a very important task because not only is it beautiful and simple and intuitive but it turns out it's incredibly powerful to build up the very very powerful language models that we see today are trained on this exact task of being able to predict the next word so let's say we have this example sentence this morning I took my cat for a walk our task is given this set of words we want to be able to predict the next word and let's say we want to build a sequence model like an RNN to do this task what is our very first step any ideas breaking breaking the words into chunks yeah let's say now we've done we've broken the sentence into chunks or words what do we how can we actually build a neural network model to do this vectorize them exactly so the core consideration after we've broken this text up into words is we need a way to actually represent it to to a model right because remember all neural networks are are their function actuators or function approximators that operate on numbers right vectors and matrices and ways of representing numbers so that's exactly what we need to do if we want to have a model that takes a word in and predicts the next word out we can't just uh pass that in as words we need to ual way to represent these words as numbers in a numerical representation to be able to operate on them using a neural network and this is this notion of vectorizing the input or encoding the language for a neural network to to operate on and this is really really a core Concept in language modeling and uh neural network and machine learning modeling in general and so the solution that we're going to introduce right now is this notion of embedding which is an idea of taking an input that can be in some form like words and then transforming it into first an index that then can map to a vector of some fixed size so to break this down step by step how do we actually do this let's say we have our VOC our body of all possible words we could see in all sentences we could encounter we call this a vocabulary right a corpus of words that covers all the possible words that we could encounter and this vocabulary has to have some fixed size right what we can then do is take the individual words in this vocabulary and map them to a number an index let's say a maps to one cat maps to two so on and so forth then now these indices give us a way to transform that index that slot to a vector and look up a vector that represents that word based on the index and I'll show you exactly what I mean by this the last step is to do this embedding operation which means mapping an index to a vector of fixed size one way we could do this is what we call a one hot or binary encoding or embedding and what I've done here is we've defined a vector of a sparse Vector of a fixed size and all it has is zeros and a one in the index that corresponds to that word and So based on that index value I can effectively encode the identity of the word and look up and backtrack what the word was based on on that index the other thing I could do is to actually use a neural network layer to learn an embedding of those words in some fixed length lower dimensional space and this is very similar we've just done this operation of mapping that index to an encoding such that similar words end up in a similar space of this embedding space but we are still able to retrieve our Vector representation of the word using that index so I think a really good way of thinking about these vectorization or um embedding operations is to think about indexing to look up a fixed representation a numerical representation for these words and this is a really really important concept so now we can do that for our sequences we can transform our words into this vector representation and to now think about okay why why sequence modeling is difficult and complex we can think and see some examples of how that really comes to life right we can have the the um complexities that arise when we think about variable sequence lengths so maybe we could have short sequences medium length sequences longer sequences and in all cases we want our neural network model to be able to track these dependencies consistently to still be able to do a good job of PR of predicting the next word at the end and that relates as well to a good ability to track and store these long-term dependencies in these sequences where in many cases we may need information from very very beginning of the sentence to predict the next word at the very end of the sentence that's important because depending on how we Shuffle the words around right this notion of sequence can convey very different semantic meanings in our prediction task and so hopefully this example of language modeling and next word prediction gives you a sense of why sequence modeling can be very rich and very complex as as a neural network deep learning task so okay that gives us away concretely in the real world some some understanding of why sequence modeling uh is Rich and challenging to actually train a sequence model like an RNN though we need some special considerations we're still going to use that fundamental algorithm that Alexander introduced of back propagation but now we need to introduce something to be able to handle that time dependence so like we've been doing throughout this lecture let's build up from the first principles that we started with let's go back to how we train our feed forward models right we first take our inputs we make a forward pass through the network going from input to Output to actually train the model and compute loss and back propagate gradients we do this backwards through the back propagation algorithm by taking the derivative of the loss with respect to each of the parameters and each of the model weights in our Network and then we adjust and move those parameters in the direction that will minimize the loss right we take a step in rnns right we saw this preview of how the loss is actually computed time step by time step now when we uh when we want to go to train rnns we not only need to consider consider an individual loss but actually the aggregate loss through all these individual time steps what that means is that instead of back propagating these loss values through a single feedforward Network we need to back propagate errors across these individual time steps and then uh such that we can carry through errors from late time steps all the way back to the beginning and the algorithm for doing this is what we call back propagation Through Time such that errors flow from all the way back late in this in the sequence to the very beginning what this means practically is that you can have this chain of very repeated computations multiplying a weight Matrix uh uh multiple times repeated with each other as well as repeated use of the derivative of the activation functions in these networks that can pose some very practical challenges which for the sake of time we're not going to go too deep into but the important thing that to keep in mind is that these standard rnns can be somewhat difficult to train stably because you could get many values that are larger than one you multiply them together and then the the gradient explodes right conversely you could have many values that are very small you multiply them together and then the gradient vanishes and it goes down to very very close to zero this actually has real practical implications because what it means is that it makes it really really hard to be able to track these long-term dependencies that we care about in our sequence if our gradients are unstable either blowing up or shrinking down to nothing we can't effectively take those gradients from late time steps and pass them through to the earlier time steps to be able to uh promote our model to retain that information and so in the literature and in the sequence modeling Community there's been a lot of active research to come up with improved architectures based on the RNN to try to solve this problem and the real core concept is that they add some complexity to that RNN unit itself effectively adding um additional functions to try to selectively control the amount of information that's passed to the update of the Hidden State one very very prominent way of doing this is a network called an lstm or long short-term memory Network and this was introduced now um quite some time ago but has been very foundational to a lot of the sequence modeling work that's gone on since so to give you one quick look at some of the applications of rnns that you'll actually get Hands-On with today I want to highlight this particular example of Music generation and this is a problem that really lends naturally to sequence modeling and to an recurrent neural network architecture because what we're doing is let's say we want to try to predict and generate a new piece of music one way we could do this is by taking the individual notes in a piece of music and very similar to how we saw with the task of predicting the next word build a model that given the past history of musical notes learns to predict the most likely next musical note in the sequence and this is exactly what you will do today in our soft sofware Labs where you'll be able to train a RNN model to generate brand new music that's never existed before and in fact you know you're you're not the only ones who have had a go at this this is an example from a few years ago of a startup that was trying to seek to do a music generation and they trained a neural network model on um classical music and tested to finish a work by the composer France Schubert uh the famous unfinished Symphony where they gave two movements of the symphony and tasked the model to generate the music corresponding to the third third movement so let's see if we can play this [Music] so it's it's pretty good maybe some there are some classical music officios in the audience who are familiar with this work but I always appreciate it because it kind of gets at some of these themes that we're talking about in terms of the um capabilities of these sequence models so so far we've talked exclusively about this sequence modeling architecture called recurrent neural networks but and I want to take a moment to kind of appreciate that it's very very remarkable that we're able to understand the fundamentals of sequence modeling and build up to some of these capabilities using rnns but like any technology and any method right rnns have some core limitations and in fact those limitations have motivated new development of new architectures and approaches in sequence modeling as well as improved versions of rnns that have done better and uh solved some of these limitations couple things that are important to keep in mind when thinking about rnns one is that the core concept as we talked about is this notion of the state hftt remember right the Concepts in all that we're talking about these neural networks operate on vectors and matrices of numbers just like that the state of an RNN is a fixed length Vector right there's only so much information that can be encapsulated into something of fixed size and so that presents what we think of as a bottleneck to the amount of information that the RNN State can hold additionally because rnn's process information time step by time step this can make them very difficult to parallelize such that things can be processed simultaneously right we have this inherent sequence dependence and finally related to both these points that encoding bottleneck of the state can prevent the long-term memory capacity of some of these types of recurrent architectures so to think about now how we can try to overcome this let's go back to our fundamental goal of sequence modeling which is to take a sequence of inputs use a neural network to compute some features or States representing those inputs and now be able to generate predictions according to that sequence with RNN we said we are going to process this time step by time step use recurrence but we also saw that inherently ly this notion of sequence time step by time step processing places some real limitations on the capabilities of RNN ultimately and ideally we want to be able to process our sequence very efficiently in parallel perhaps so that we can generalize uh to Long sequences and do so efficiently and also have this desired attribute of being able to track these important dependencies in the sequence effectively and so a question that was posed a few years ago is what if we could try to tackle the sequence modeling problem without having to deal with the data time step by time step maybe we can eliminate the need for recurrence maybe we could do this by squashing everything together right ignore this notion of these individual time steps and let's say concatenate all our inputs together right into one vector and we feed it into something like a feed forward model and generate an output at the end and hope that it makes sense while we if we do this in this naive first approach right yes we've eliminated the need for recurrence so we don't have to process our data step by step that's good but this doesn't seem really scalable because now let's say we're just trying to use a dense Network that's not going to be very efficient also we've destroyed all information about order we've squashed the inputs into a concatenated vector and by doing so we've destroyed any hope to remember things that appeared earlier or later and relate them to each other so this has has motivated a different way of thinking about sequence modeling which is this idea of trying to take a representation of sequential data and Define a mechanism that can on its own pick out and look at the parts of that information that are important relative to other uh parts of that thinking about this in other words can we Define a way to be able to given a sequence identify and attend to the important parts of that sequence and moreover model the dependencies in that sequence that relate to each other and this is the core idea of a very powerful mechanism called attention and in 2017 there was a paper introduced called attention is all you need that introduced this mechanism and so if you've heard of a model like chat GPT or GPT the T T in that acronym stands for a Transformer and a Transformer is a type of neural network architecture that can be applied not only to you know language data but other types of sequential data and the foundational mechanism of a Transformer what makes it different is this operation of attention and so we're going to in this lecture talk about that attention mechanism and in later lectures you will learn more about how these Transformers are actually being applied as language models and in other applications as well so we're going to break down the core concept of attention really step by step all right let's do that okay attention itself is a very informative word right what it means is we have this inherent ability as humans to to think about an input and automatically zoom in and pick out the things that are Salient the important features so let's build up our intuition starting with an image how do we figure out what's important in this image one way we could do it naively is we could go pixel by pixel left to right back and forth and scan this to try to compute some value about how important these individual pixels are but of course our brains don't operate like that we're able to automatically look at this and pick out to and attend to the important parts and the first part of this problem is this notion of being able to identify which parts are are important in some input and then ultimately we want to use that identification to extract the features the components of the input that correspond to these high attention values right and to think about this more concretely this notion of identifying the parts to attend to is really similar to search right so when we do something like a search we present a question and we're trying to seek an answer let's say you came to this class right and you had the question how can I learn more about neurl Networks and deep learning and AI maybe one thing you could do besides coming to this class would be to go to the Internet have all the videos all the materials available on the Internet and try to do a search to find something that matches your query let's say you go to a giant video database like YouTube and you put in your query your ask deep learning that's your search topic and now what search does is through let's say we go through every video in this database and we ex extract some informative nugget of information a key that represents the core element of that video a descriptor of that video and now we have our query and we have a set of keys now our task is okay to actually do the search how could I go about this well I want to see how close the match is between my query what I searched for and those keys those key indicators in the database and I want to see how similar they are so I'll do this step by step ask how similar is my query to these Keys first instance a beautiful video about elegant sea turtles not similar a video of from our past lecture on deep learning yes similar a key related to Kobe Bryant's fadeaway not similar so now we've identified the key that's important we want to attend to this our last task is to actually extract the value that's associated with this similar uh match that we found we want to extract the features that we want to pay attention to the video itself and we'll call this the value and because our search was implemented with a good attention mechanism we've identified the best deep learning course out there for you and your query this concept is really the core intuition behind attention and that's very very closely related to how this attention operation Works in neural networks like Transformers so now let's go back to our sequence modeling problem where we have a series of words and we want to predict the next word with this sentence if we break this down step by step first remember we don't want to process the information time step by time step we've broken down that need for recurrence we're going to feed in the data all at once and still we need a way to encode some notion about order so what we're going to do is we're going to put in an embedding called a positional embedding that effectively gives us a way to encapsulate that relative position of these elements in that sequence we're not going to go into great detail about positional embeddings but you can think of it as an encoding that gives us some representation of position in the sequence now we take those position aware encodings and we do that search operation and we operationalize that search operation to extract three sets of matrices that we call the query the key and the value just like I introduced before the way we do that is we take the positional embedding and think about that as representing our sequence in a positionally aware way and we use a neural network layer to produ uce each of these individual matrices the query the key and the value the important thing to keep in mind is that it's that same positional embedding that's repeated in this mechanism of self attention but these are different neural network layers that yield different values for each of the query key and value matrices that means is that they can effectively capture different information as well see now again our next step in the search operation was to figure out okay how similar is the query to the key and that's exactly what we're going to do next with attention to do that right remember these are um numeric vectors or matrices and so we need a mathematical way to compute that similarity between two sets of numerical features so let's say we have two vectors our query Vector our key vector and what we can do mathematically using linear algebra is measure the similarity of those vectors in space using the dot product that tells us how close they are to each other we can scale it and this gives us a very concrete similarity metric that captures that similarity between the qu query and the key that same principle can apply now to matrices as well do product and scaling that gives us a similarity metric now let's think about what this similarity computation actually means remember right we're trying to understand how these components of the input relate to each other what parts of the sentence are important to each other and uh important to convey the semantic meaning of the sentence as a whole so if we have this examp example he tossed the tennis ball to serve let's say we have a we've computed our querying key matrices applied a scaling we can then apply a function called a softmax to basically squish these values between zero and one and now this gives us a matrix that gives us a relative waiting of how those individual components in the sequence relate to one another intuitively you can think about things that that are more similar as having higher attention weights more related as having higher attention weights things that are less related as having lower attention weights so here right in this example toss and ball have a high score tennis and ball have a high score and so on this gives us our attention waiting or attention Matrix the final step is to now use that relative waiting to actually pull out those important Fe features that we care about what we do here is we take our value Matrix multiply it by our attention weight and this gives us an output of a feature set over that input space that reflects the relative elements of the sequence that are interrelated relative to each other that's really the core of how attention works and that's really really be it's really really beautiful and striking to me because this mechanism gives us a very natural way to pull out and attend to features that are quite important relative to each other in an input and architecturally now how do we actually build this out into something like a Transformer well one second please um we can go again by taking our input Computing these positional encodings we Define these neural network layers that compute the query key and value then we can compute this relative waiting between the query and the key that's a matrix multiply representing the dot product a scaling and a softmax and then ex use the value Matrix to extract features that have high attention scores and these are the core operations that now Define these attention heads which can be which are really the core component of architectures uh like the transformer so as as we've mentioned and realized right this is really the the foundational attention is the foundational building block of of Transformers and as I think kind of some questions got to as well right a Transformer architecture does not need to be defined by just a single attention block you can actually stack multiple attention heads uh together to now be able to uh basically increase the capacity of your network and pull out different sets of features and more complex sets of features so again in this very intuitive example maybe you have a Network that has three attention heads and if you go in and inspect again this is a intuitive um example let's say you were to go in and inspect the values of each of these attention heads maybe you could get some interpretability out with respect ECT to the different features or parts of the input that the network was attending to so what are some real world use cases and what has attention really enabled and transformed over the recent years it's not only in language processing that Transformers and self- attention have really led to tremendous advances while that's the case right the mechanism and the architecture behind attention Transformers is very generalizable and so as you'll see right natural language is one tremendous area that Transformers have really taken off and you'll get hands-on experience with this not only um in the lectures but in a brand new software lab on llms we'll also see a bit about how attention and sequence modeling has been extended to biological sequences in one of our guest lectures and in fact in something that may not appear sequential like images or computer vision there are a class of models called Vision Transformers that have now also become very very powerful um in processing image data as well so to summarize and and close hopefully you've gotten a sense of how rich sequence modeling is as a set of problems and things we can consider we saw how rnn's work we saw how we can build up intuition for rnns through this notion of recurrence how they can be trained through back propagation you'll be able to get a hands-on experience building rnn's for music generation and finally we closed with talking about self- atttention and Transformers as a way to model sequences uh without needing to handle time steps individually and recurrently and stay tuned for more on llms both Hands-On and in more lectures so with that that closes the lecture portion for today we now can use the remaining time for open Office hours and discussion about any lingering questions you have or comments uh related to the discussion we also want to draw your attention to the software Labs which are now available on the GitHub linked on the course website the instructions for completing the software labs are all there we have options in both tensorflow and pytorch so hopefully you'll get um a fun chance to go through those and work with those and finally I think um our gracious host reception host John stepped out but immediately uh after this there will be a in-person reception to kick off the uh the course just down the street at One Kendall Square um with food provided and many special thanks to John Werner and Link Venture oh he's still there back at the top thank you John for for graciously hosting